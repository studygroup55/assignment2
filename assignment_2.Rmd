
During this assignment you pick the model you have been working on and write a report:

- describing the model (you can re-use text from assignment 1, if relevant)
- showcasing a commented version of the stan model (what does each line do?)
- describing a process of parameter recovery (why are you doing it?, how are you doing it?)
- discussing the results: how many trials should be used at least to properly recover the parameters? what is the role of priors? Add relevant plot(s).

N.B. to pass it's enough to use the single agent model, but if you want to learn more (at the cost of more time investment), you could
- Build the multilevel version of your model and run the parameter recovery on it.
- Analyze an empirical dataset.

Empirical datasets:
- Guide to the data: https://www.dropbox.com/s/m1u5roo4z0f2emr/readme.txt?dl=0
- Non-human primates dataset: https://www.dropbox.com/s/8fj7k5jk8mn4j63/mp_primates.csv?dl=0
- Human primates dataset (former CogSci'ers): https://www.dropbox.com/s/9z6hcg9b4yhzgbu/mp_students.csv?dl=0
https://www.dropbox.com/s/r2ok23vjewyelaz/mp_students_22.csv?dl=0
- Clinical dataset (schizophrenia and controls): https://www.dropbox.com/s/dxj9f8txg1yax68/mp_schizophrenia.csv?dl=0

```{r}
library(rstan)
library(tidyverse)
library(reshape2)
library(ggdag)
library(ggplot2)
library(cmdstanr)
theme_set(theme_dag())
```

https://ourcodingclub.github.io/tutorials/stan-intro/
https://vasishth.github.io/bayescogsci/book/ch-introstan.html


**TO DO**

- Make a new model apart from the WSS-LSS one
- Create DAGs for both models https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html 


```{r}
dagify(choice ~ x) %>%
  ggdag()
```


**TEST: For single game between WSLS / random**

First, simulate the data

```{r}
#Defining agent function
WSLSAgent_f <- function(prevChoice, Feedback, noise) {
  if (Feedback == 1) {      #If feedback = 1 (win), stay
    choice = prevChoice
  }
  else if (Feedback == 0) { #If feedback = 0 (loss), shift
    choice = 1 - prevChoice
  }
  if(rbinom(1,1,noise)==1){
    choice <- rbinom(1,1,0.5)
  }
  return(choice)
}

#Defining agent function
RandomAgent_f <- function(rate){
  choice <- rbinom(1, 1, rate)
  return(choice)
}

#Define rate
rate <- 0.5

#Define noise
noise <- 0.3

#Define number of trials
trials = 120

#Define empty vectors
Self <- rep(NA, trials)
Other <- rep(NA, trials)

#Define random first choice for player
Self[1] <- RandomAgent_f(0.5) #player

#Run simulation
for (t in seq(trials)){Other[t] <- RandomAgent_f(rate)}
for (i in 2:trials){
  if (Self[i-1] == Other[i-1]){
    Feedback = 1
  } else {Feedback = 0}
  Self[i] <- WSLSAgent_f(Self[i-1], Feedback, noise)
}

#show
df <- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other))
```


**Transform the data to the appropriate form**

```{r}
#Firstly, we need to transform feedback into a variable capturing the rate of which an agent chooses right. That means that we code every left choice as -1 and every right choice as 1 (thus centering equal distribution of left/right choices to 0).

#NAs in feedback
df$Feedback <- NA

#Transform feedback
df$Feedback[df$Other == 1 & df$Self == 1] <- 1 #self wins on right, goes right
df$Feedback[df$Other == 1 & df$Self == 0] <- 1 #self loses on left, goes right
df$Feedback[df$Other == 0 & df$Self == 1] <- 0 #self loses on right, goes left
df$Feedback[df$Other == 0 & df$Self == 0] <- 0 #self wins on left, goes left

#Transform to list
data <- list(
  n_trials = nrow(df),  # n of trials
  choice = df$Self, # sequence of choices
  next_choice = df$Feedback # whether or not agent is choosing right 
)

data
```


**Compile and fit the model**


```{r}
### DIFFERENT SYNTAX THAN RICCARDO ###

model_code <- readLines("matching_pennies_test.stan")
model <- stan_model(model_code = model_code)

fit <- sampling(model, data, chains = 4, iter = 2000, warmup = 1000)
summary(fit)
```


**SEE ISSUES NOTED WITHIN STAN CODE MATCHING_PENNIES_TEST** 

```{r}
### Riccardo's syntax ###

#collect stan model
file <- file.path("~/Desktop/MA/Semester_2/Advanced_Cognitive_Modelling/assignment_2/assignment2/matching_pennies_test.stan")
mod <- cmdstan_model(file, 
                     # this specifies we can parallelize the gradient estimations on multiple cores
                     cpp_options = list(stan_threads = TRUE), 
                     # this is a trick to make it faster
                     stanc_options = list("O1")) 

#extract samples
samples <- mod$sample(
  data = data, # the data :-)
  seed = 123,  # a seed, so I always get the same results
  chains = 2,  # how many chains should I fit (to check whether they give the same results)
  parallel_chains = 2, # how many of the chains can be run in parallel?
  threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
  iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
  iter_sampling = 2000, # total number of iterations
  refresh = 0,  # how often to show that iterations have been run
  max_treedepth = 20, # how many steps in the future to check to avoid u-turns
  adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
)

#extract summary 
samples$summary()

# Extract posterior samples and include sampling of the prior:
draws_df <- as_draws_df(samples$draws())

# Checking the model's chains (noise)
ggplot(draws_df, aes(.iteration,  noise, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()

# Checking the model's chains (theta)
ggplot(draws_df, aes(.iteration, theta, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()


 #### EXPERIMENTAL ####


# add a prior for theta (ugly, but we'll do better soon)
draws_df <- draws_df %>% mutate(
  theta_prior = rbeta(nrow(draws_df), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df) +
  geom_density(aes(theta), fill = "blue", alpha = 0.3) +
  geom_density(aes(theta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.3, linetype = "dashed", color = "black", size = 1.5) + #change intercept according to noise parameter
  xlab("Theta (rate)") +
  ylab("Posterior Density") +
  theme_classic()


# add a prior for noise (ugly, but we'll do better soon)
draws_df <- draws_df %>% mutate(
  noise_prior = rbeta(nrow(draws_df), 1, 1)
)

# Now let's plot the density for noise (prior and posterior)
ggplot(draws_df) +
  geom_density(aes(noise), fill = "blue", alpha = 0.3) +
  geom_density(aes(noise_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.3, linetype = "dashed", color = "black", size = 1.5) + #change intercept according to noise parameter
  xlab("Noise") +
  ylab("Posterior Density") +
  theme_classic()

```


















