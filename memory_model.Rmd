---
title: "memory_model.Rmd"
output: html_document
date: "2023-03-14"
---
Implementation of a simple memory model, parameter recovery, and testing of trial number + priors. (commented pr line in Stan)

Including multilevel (parameter recovery) and/or empirical data (analyze) in the report.

Empirical datasets:
- Guide to the data: https://www.dropbox.com/s/m1u5roo4z0f2emr/readme.txt?dl=0
- Non-human primates dataset: https://www.dropbox.com/s/8fj7k5jk8mn4j63/mp_primates.csv?dl=0
- Human primates dataset (former CogSci'ers): https://www.dropbox.com/s/9z6hcg9b4yhzgbu/mp_students.csv?dl=0
https://www.dropbox.com/s/r2ok23vjewyelaz/mp_students_22.csv?dl=0
- Clinical dataset (schizophrenia and controls): https://www.dropbox.com/s/dxj9f8txg1yax68/mp_schizophrenia.csv?dl=0

**Testing agent simulation** 

```{r}
#Loading packages
pacman::p_load("rstan", "tidyverse", "reshape2", "ggdag", "cmdstanr", "posterior", "brms")
```


**Defining the agents**

```{r}
#Define a random agent
RandomAgent_f <- function(rate){
  choice <- rbinom(1, 1, rate)
  
  if (choice == 0){
    choice = -1
  }
  return(choice)
}

#Another suggestion for a memory agent
EMA_Agent_f <- function(beta, bias, memory, n_trials){
  
  choice <- rbinom(1,1, inv_logit_scaled(bias + beta * memory))
  
  if (choice == 0){
    choice = -1
  }
  return(choice)
}
```

**Simulation**
```{r}
#Simulating the memory agent data

rate <- 0.7
alpha <- 0.3
beta <- 1
bias <- 0.5
n_trials <- 120

#Define empty vectors
Self <- rep(NA, n_trials)
Other <- rep(NA, n_trials)

#Simulate data
#Other - random agent
for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}
#Self - memory agent
memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)

for (i in 1:n_trials){
  Self[i] <- EMA_Agent_f(beta, bias, memory[i], n_trials)
  memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
}

```


**Transforming the data to the appropriate form**
```{r}

memory <- memory[1:120]

#Create df
df <- tibble(Self, Other, memory, trial = seq(n_trials))


#Making Self_choice 0 and 1, cause we need that in the bernoulli in the stan model
df <- df %>% 
  mutate(Self = ifelse(Self == -1, 0, Self))



#Transform data to lists
data <- list(
  n_trials = nrow(df),  # n of trials
  choice = df$Self, # sequence of choices
  other = df$Other # sequence of the other agents choices
    )
```


**Compile and fit the model**
```{r}
file <- file.path("memory_agent.stan")
  mod <- cmdstan_model(file, 
                       # this specifies we can parallelize the gradient estimations on multiple cores
                       cpp_options = list(stan_threads = TRUE), 
                       # this is a trick to make it faster
                       stanc_options = list("O1")) 
  
  #extract samples
  samples <- mod$sample(
    data = data, # the data :-)
    seed = 123,  # a seed, so I always get the same results
    chains = 2,  # how many chains should I fit (to check whether they give the same results)
    parallel_chains = 2, # how many of the chains can be run in parallel?
    threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
    iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
    iter_sampling = 2000, # total number of iterations
    refresh = 0,  # how often to show that iterations have been run
    max_treedepth = 20, # how many steps in the future to check to avoid u-turns
    adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
  )
  
  #extract summary 
  samples$summary()

  
  # Extract posterior samples and include sampling of the prior:
  draws_df <- as_draws_df(samples$draws())

  
  # Checking the model's chains (noise)
  ggplot(draws_df, aes(.iteration, alpha, group = .chain, color = .chain)) +
      geom_line() +
      theme_classic()

```


*Function for simulating with different values for alpha*

```{r}
list_of_dfs <- list()
temp <- list()
recovery_df <- data.frame()
#n_trials = 500


for(alpha_lvl in seq(0.2, 0.8, 0.2)){
  set.seed(1982)
  
  alpha = alpha_lvl
  n_trials = 120
  rate <- 0.7 #Probability scale
  beta <- 1 #logodds
  bias <- 0.5 #NB this is on a logodds scale, so it has a small bias towards right 
  
  #Define empty vectors
  Self <- rep(NA, n_trials)
  Other <- rep(NA, n_trials)
  
  #Run simulation
  for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}
  
  #Self - memory agent
  memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)
  
  for (i in 1:n_trials){
    Self[i] <- EMA_Agent_f(beta, bias, memory[i], n_trials)
    memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
  }
  
  
  memory <- memory[1:n_trials]
  
  #Create df
  df <- tibble(Self, Other, memory, trial = seq(n_trials))
  
  
  #Making Self_choice 0 and 1, cause we need that in the bernoulli in the stan model
  df <- df %>% 
    mutate(Self = ifelse(Self == -1, 0, Self))
  
  
  
  #Transform data to lists
  data <- list(
    n_trials = nrow(df),  # n of trials
    choice = df$Self, # sequence of choices
    other = df$Other # sequence of the other agents choices
      )
  
  
  
  file <- file.path("memory_agent.stan")
  mod <- cmdstan_model(file, 
                       # this specifies we can parallelize the gradient estimations on multiple cores
                       cpp_options = list(stan_threads = TRUE), 
                       # this is a trick to make it faster
                       stanc_options = list("O1")) 
  
  #extract samples
  samples <- mod$sample(
    data = data, # the data :-)
    seed = 123,  # a seed, so I always get the same results
    chains = 2,  # how many chains should I fit (to check whether they give the same results)
    parallel_chains = 2, # how many of the chains can be run in parallel?
    threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
    iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
    iter_sampling = 2000, # total number of iterations
    refresh = 0,  # how often to show that iterations have been run
    max_treedepth = 20, # how many steps in the future to check to avoid u-turns
    adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
  )
  
  #extract summary 
  samples$summary()
  
  # Extract posterior samples and include sampling of the prior:
  draws_df <- as_draws_df(samples$draws())

  
  # Checking the model's chains (noise)
  temp = ggplot(draws_df, aes(.iteration, alpha, group = .chain, color = .chain)) +
      geom_line() +
      theme_classic()
  
  # assign function within loop
  assign(paste0("plot_", alpha_lvl), temp)

  assign(paste0("draws_df_", alpha_lvl), draws_df)
  
  recovery_df <- rbind(recovery_df, samples$summary("alpha"), samples$summary("beta"), samples$summary("bias"))
  
}
```


**Plots for different values of alpha**
```{r}
#install.packages("ggpubr")
library(ggpubr)


plot_0.2 <- plot_0.2 + ggtitle("Alpha = 0.2")

# add a prior for theta (ugly, but we'll do better soon)
draws_df_0.2<- draws_df_0.2 %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.2), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
p1 <- ggplot(draws_df_0.2) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.2, linetype = "dashed", color = "black", size = 1) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  ylim(0,3)+
  theme_classic()+
  ggtitle("True alpha = 0.2")



plot_0.4 <- plot_0.4 + ggtitle("Alpha = 0.4")

# add a prior for theta (ugly, but we'll do better soon)
draws_df_0.4<- draws_df_0.4 %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.4), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
p2 <- ggplot(draws_df_0.4) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.4, linetype = "dashed", color = "black", size = 1) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  ylim(0,3)+
  theme_classic()+
  ggtitle("True alpha = 0.4")


plot_0.6 <- plot_0.6 + ggtitle("Alpha = 0.6")

# add a prior for theta (ugly, but we'll do better soon)
draws_df_0.6<- draws_df_0.6 %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.6), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
p3 <- ggplot(draws_df_0.6) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.6, linetype = "dashed", color = "black", size = 1) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  ylim(0,3)+
  theme_classic()+
  ggtitle("True alpha = 0.6")


plot_0.8 <- plot_0.8 + ggtitle("Alpha = 0.8")

# add a prior for theta (ugly, but we'll do better soon)
draws_df_0.8 <- draws_df_0.8 %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.8), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
p4 <- ggplot(draws_df_0.8) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.8, linetype = "dashed", color = "black", size = 1) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  ylim(0,3)+
  theme_classic()+
  ggtitle("True alpha = 0.8")
  


ggarrange(plot_0.2 + rremove("x.text")+ rremove("xlab"), plot_0.4 + rremove("x.text")+ rremove("xlab")+ rremove("y.text") + rremove("ylab"), plot_0.6, plot_0.8 + rremove("y.text")+ rremove("ylab"),
          align = "hv",
          padding = 4,
          ncol = 2, nrow = 2)

ggarrange(p1 + rremove("x.text")+ rremove("xlab"), p2 + rremove("x.text")+ rremove("xlab")+ rremove("y.text") + rremove("ylab"), p3, p4 + rremove("y.text")+ rremove("ylab"),
          align = "hv",
          padding = 4,
          ncol = 2, nrow = 2)

p4
?rremove

```


```{r}
plot_0.2

#ALPHA
# add a prior for theta (ugly, but we'll do better soon)
draws_df<- draws_df %>% mutate(
  alpha_prior = rbeta(nrow(draws_df), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.2) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.2, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  theme_classic()



#BIAS
# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.2) +
  geom_density(aes(bias), fill = "blue", alpha = 0.3) +
  geom_density(aes(bias_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Bias") +
  ylab("Posterior Density") +
  theme_classic()


#BETA
# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.2) +
  geom_density(aes(beta), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.9, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  theme_classic()
```



**Looking at the recoveries**
```{r}
recovery_df
```


**Function for simulating and saving data**
```{r}
list_of_dfs <- list()
temp <- list()
recovery_df <- data.frame()
#n_trials = 500

  for(beta_lvl in seq(-1.5, 1.5, 1)){
    set.seed(1982)
    
    beta = beta_lvl
    n_trials = 120
    rate <- 0.7
    alpha <- 0.3 #Cognitively realistic according to the literature 
    bias <- 0.5 #NB this is on a logodds scale, so it has a small bias towards right 
    
    #Define empty vectors
    Self <- rep(NA, n_trials)
    Other <- rep(NA, n_trials)
    
    #Run simulation
    for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}
    
    #Self - memory agent
    memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)
    
    for (i in 1:n_trials){
      Self[i] <- EMA_Agent_f(beta, bias, memory[i], n_trials)
      memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
    }
    
    
    memory <- memory[1:n_trials]
    
    #Create df
    df <- tibble(Self, Other, memory, trial = seq(n_trials))
    
    
    #Making Self_choice 0 and 1, cause we need that in the bernoulli in the stan model
    df <- df %>% 
      mutate(Self = ifelse(Self == -1, 0, Self))
    
    
    
    #Transform data to lists
    data <- list(
      n_trials = nrow(df),  # n of trials
      choice = df$Self, # sequence of choices
      other = df$Other # sequence of the other agents choices
      )
    
    
    
    file <- file.path("memory_agent.stan")
    mod <- cmdstan_model(file, 
                         # this specifies we can parallelize the gradient estimations on multiple cores
                         cpp_options = list(stan_threads = TRUE), 
                         # this is a trick to make it faster
                         stanc_options = list("O1")) 
    
    #extract samples
    samples <- mod$sample(
      data = data, # the data :-)
      seed = 123,  # a seed, so I always get the same results
      chains = 2,  # how many chains should I fit (to check whether they give the same results)
      parallel_chains = 2, # how many of the chains can be run in parallel?
      threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
      iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
      iter_sampling = 2000, # total number of iterations
      refresh = 0,  # how often to show that iterations have been run
      max_treedepth = 20, # how many steps in the future to check to avoid u-turns
      adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
    )
    
    #extract summary 
    samples$summary()
    
    # Extract posterior samples and include sampling of the prior:
    draws_df <- as_draws_df(samples$draws())
    
    temp2 <- tibble(betaEst = (draws_df$beta),
                    betaTrue = beta_lvl,
                    biasTrue = bias_lvl)
    
    if (exists("recovery_df_2")) {recovery_df_2 <- rbind(recovery_df_2, temp2)} else {recovery_df_2 <- temp2}
  
    
    # Checking the model's chains (noise)
    temp = ggplot(draws_df, aes(.iteration, beta, group = .chain, color = .chain)) +
        geom_line() +
        theme_classic()
    
    # assign function within loop
    assign(paste0("plot_", inv_logit_scaled(beta_lvl)), temp)
  
    assign(paste0("draws_df_", inv_logit_scaled(beta_lvl)), draws_df)
    
    
    
    recovery_df <- rbind(recovery_df, samples$summary("alpha"), samples$summary("beta"), samples$summary("bias"))
  }  
```


**Nice plots**
```{r}

plot_0.182425523806356 <- plot_0.182425523806356 + ggtitle("Beta ~ 0.2")
#BETA
# Now let's plot the density for theta (prior and posterior)
p5 <- ggplot(draws_df_0.182425523806356) +
  geom_density(aes(inv_logit_scaled(beta)), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.2, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  theme_classic()+ 
  ylim(0,10)+
  ggtitle("True beta ~ 0.2")

plot_0.377540668798145 <- plot_0.377540668798145 + ggtitle("Beta ~ 0.4")
#BETA
# Now let's plot the density for theta (prior and posterior)
p6<- ggplot(draws_df_0.377540668798145) +
  geom_density(aes(inv_logit_scaled(beta)), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.4, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  ylim(0,10)+
  theme_classic()+ 
  ggtitle("True beta ~ 0.4")


plot_0.622459331201855 <- plot_0.622459331201855 + ggtitle("Beta ~ 0.6")
#BETA
# Now let's plot the density for theta (prior and posterior)
p7 <- ggplot(draws_df_0.622459331201855) +
  geom_density(aes(inv_logit_scaled(beta)), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.6, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  theme_classic()+ 
  ylim(0,10)+
  ggtitle("True beta ~ 0.6")


plot_0.817574476193644 <- plot_0.817574476193644 + ggtitle("Beta ~ 0.8")
#BETA
# Now let's plot the density for theta (prior and posterior)
p8 <- ggplot(draws_df_0.817574476193644) +
  geom_density(aes(inv_logit_scaled(beta)), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.8, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  theme_classic()+ 
  ylim(0,10)+
  ggtitle("True beta ~ 0.8")



library(ggpubr)

ggarrange(plot_0.182425523806356 + rremove("x.text")+ rremove("xlab"), plot_0.377540668798145 + rremove("x.text")+ rremove("xlab")+ rremove("y.text") + rremove("ylab"), plot_0.622459331201855, plot_0.817574476193644 + rremove("y.text")+ rremove("ylab"),
          align = "hv",
          padding = 4,
          ncol = 2, nrow = 2)

ggarrange(p5 + rremove("x.text")+ rremove("xlab"), p6 + rremove("x.text")+ rremove("xlab")+ rremove("y.text") + rremove("ylab"), p7, p8 + rremove("y.text")+ rremove("ylab"),
          align = "hv",
          padding = 4,
          ncol = 2, nrow = 2)



```



**Same function as above, but with an outer loop for bias**
```{r}
list_of_dfs <- list()
temp <- list()
recovery_df <- data.frame()
#n_trials = 500

for (bias_lvl in seq(0, 1.5, 0.5)){
  for(beta_lvl in seq(-1.5, 1.5, 1)){
    set.seed(1982)
    
    beta = beta_lvl
    n_trials = 120
    rate <- 0.7
    alpha <- 0.3 #Cognitively realistic according to the literature 
    bias <- bias_lvl 
    
    #Define empty vectors
    Self <- rep(NA, n_trials)
    Other <- rep(NA, n_trials)
    
    #Run simulation
    for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}
    
    #Self - memory agent
    memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)
    
    for (i in 1:n_trials){
      Self[i] <- EMA_Agent_f(beta, bias, memory[i], n_trials)
      memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
    }
    
    
    memory <- memory[1:n_trials]
    
    #Create df
    df <- tibble(Self, Other, memory, trial = seq(n_trials))
    
    
    #Making Self_choice 0 and 1, cause we need that in the bernoulli in the stan model
    df <- df %>% 
      mutate(Self = ifelse(Self == -1, 0, Self))
    
    
    
    #Transform data to lists
    data <- list(
      n_trials = nrow(df),  # n of trials
      choice = df$Self, # sequence of choices
      other = df$Other # sequence of the other agents choices
      )
    
    
    
    file <- file.path("memory_agent.stan")
    mod <- cmdstan_model(file, 
                         # this specifies we can parallelize the gradient estimations on multiple cores
                         cpp_options = list(stan_threads = TRUE), 
                         # this is a trick to make it faster
                         stanc_options = list("O1")) 
    
    #extract samples
    samples <- mod$sample(
      data = data, # the data :-)
      seed = 123,  # a seed, so I always get the same results
      chains = 2,  # how many chains should I fit (to check whether they give the same results)
      parallel_chains = 2, # how many of the chains can be run in parallel?
      threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
      iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
      iter_sampling = 2000, # total number of iterations
      refresh = 0,  # how often to show that iterations have been run
      max_treedepth = 20, # how many steps in the future to check to avoid u-turns
      adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
    )
    
    #extract summary 
    samples$summary()
    
    # Extract posterior samples and include sampling of the prior:
    draws_df <- as_draws_df(samples$draws())
    
    temp2 <- tibble(betaEst = (draws_df$beta),
                    betaTrue = beta_lvl,
                    biasTrue = bias_lvl)
    
    if (exists("recovery_df_2")) {recovery_df_2 <- rbind(recovery_df_2, temp2)} else {recovery_df_2 <- temp2}
  
    
    # Checking the model's chains (noise)
    temp = ggplot(draws_df, aes(.iteration, beta, group = .chain, color = .chain)) +
        geom_line() +
        theme_classic()
    
    # assign function within loop
    assign(paste0("plot_", inv_logit_scaled(beta_lvl)), temp)
  
    assign(paste0("draws_df_", inv_logit_scaled(beta_lvl)), draws_df)
    
    
    
    recovery_df <- rbind(recovery_df, samples$summary("alpha"), samples$summary("beta"), samples$summary("bias"))
  }  
}
```

