---
title: "memory_model.Rmd"
output: html_document
date: "2023-03-14"
---

**Testing agent simulation** 

```{r}
#Loading packages
pacman::p_load("rstan", "tidyverse", "reshape2", "ggdag", "cmdstanr", "posterior", "brms")
```


**Defining the agents**

```{r}
#Define a random agent
RandomAgent_f <- function(rate){
  choice <- rbinom(1, 1, rate)
  
  if (choice == 0){
    choice = -1
  }
  return(choice)
}

#Define a memory agent based on the exponential moving average
EMA_Agent_f <- function(beta, bias, memory, n_trials){
  
  choice <- rbinom(1,1, inv_logit_scaled(bias + beta * memory))
  
  if (choice == 0){
    choice = -1
  }
  return(choice)
}
```

**Simulation**
```{r}
#Defining true parameter values and number of trials
rate <- 0.7
alpha <- 0.3
beta <- 1
bias <- 0.5
n_trials <- 120

#Define empty vectors
Self <- rep(NA, n_trials)
Other <- rep(NA, n_trials)

#Simulate data

#Other - random agent
for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}

#Self - memory agent
memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)

for (i in 1:n_trials){
  Self[i] <- EMA_Agent_f(beta, bias, memory[i], n_trials)
  memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
}

```


**Transforming the data to the appropriate form**
```{r}
#Drop first value in memory vector
memory <- memory[1:120]

#Create dataframe
df <- tibble(Self, Other, memory, trial = seq(n_trials))

#Re-code self choice to 0 and 1 (needed in the Stan file)
df <- df %>% 
  mutate(Self = ifelse(Self == -1, 0, Self))

#Transform data to lists
data <- list(
  n_trials = nrow(df), 
  choice = df$Self,
  other = df$Other)

```

**Test module: Compile and fit the model for fixed parameter values**
```{r}
# file <- file.path("memory_agent.stan")
#   mod <- cmdstan_model(file, 
#                        # this specifies we can parallelize the gradient estimations on multiple cores
#                        cpp_options = list(stan_threads = TRUE), 
#                        # this is a trick to make it faster
#                        stanc_options = list("O1")) 
#   
#   #extract samples
#   samples <- mod$sample(
#     data = data, # the data :-)
#     seed = 123,  # a seed, so I always get the same results
#     chains = 2,  # how many chains should I fit (to check whether they give the same results)
#     parallel_chains = 2, # how many of the chains can be run in parallel?
#     threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
#     iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
#     iter_sampling = 2000, # total number of iterations
#     refresh = 0,  # how often to show that iterations have been run
#     max_treedepth = 20, # how many steps in the future to check to avoid u-turns
#     adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
#   )
#   
#   #extract summary 
#   samples$summary()
# 
#   
#   # Extract posterior samples and include sampling of the prior:
#   draws_df <- as_draws_df(samples$draws())
# 
#   
#   # Checking the model's chains (noise)
#   ggplot(draws_df, aes(.iteration, alpha, group = .chain, color = .chain)) +
#       geom_line() +
#       theme_classic()

```

**Prior sensitivity checks for bias and beta** 

**Apply and compile Stan model with varying prior values**
```{r}
pacman::p_load(tidyverse,
        here,
        posterior,
        cmdstanr,
        brms, 
        tidybayes,
        future,
        purrr,
        furrr)
plan(multisession, workers = 4)

#Adding different priors
prior_mean_bias <- 0
prior_sd_bias <- seq(0.2, 0.8, 0.2)
prior_mean_beta <- 0
prior_sd_beta <- seq(0.2, 0.8, 0.2)

priors <-  tibble(expand.grid(tibble(prior_mean_bias, prior_sd_bias, prior_mean_beta, prior_sd_beta)))

sim_d_and_fit <- function(prior_mean_bias, prior_sd_bias, prior_mean_beta, prior_sd_beta) {
  
  #Transform data to lists
  data <- list(
    n_trials = nrow(df),  # n of trials
    choice = df$Self, # sequence of choices
    other = df$Other, # sequence of the other agents choices
    prior_mean_bias = prior_mean_bias,
    prior_sd_bias = prior_sd_bias,
    prior_mean_beta = prior_mean_beta,
    prior_sd_beta = prior_sd_beta
    )
    
  #Fetch model
  file <- file.path("memory_agent_test.stan")
  mod <- cmdstan_model(file, 
                       # this specifies we can parallelize the gradient estimations on multiple cores
                       cpp_options = list(stan_threads = TRUE), 
                       # this is a trick to make it faster
                       stanc_options = list("O1")) 
  
  samples <- mod$sample(
    data = data,
    seed = 1000,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 1000,
    iter_sampling = 2000,
    refresh = 0,
    max_treedepth = 20,
    adapt_delta = 0.99,
    )
  
  draws_df <- as_draws_df(samples$draws()) 
  
  temp <- tibble(bias_prior = draws_df$bias_prior, 
                 beta_prior = draws_df$beta_prior, 
                 alpha_prior = draws_df$alpha_prior,
                 
                 bias_posterior = draws_df$bias_posterior, 
                 beta_posterior = draws_df$beta_posterior,
                 alpha_posterior = draws_df$alpha_posterior,
                 
                 bias_prior_preds = draws_df$bias_prior_preds,
                 beta_prior_preds = draws_df$beta_prior_preds,
                 alpha_prior_preds = draws_df$alpha_prior_preds,
                 
                 bias_posterior_preds = draws_df$bias_posterior_preds,
                 beta_posterior_preds = draws_df$beta_posterior_preds,
                 alpha_posterior_preds = draws_df$alpha_posterior_preds,
                 
                 prior_mean_bias = prior_mean_bias,
                 prior_sd_bias = prior_sd_bias, 
                 prior_mean_beta = prior_mean_beta,
                 prior_sd_beta = prior_sd_beta)
    
    return(temp)
  
}

#Compile model for different priors
recovery_df <- future_pmap_dfr(priors, sim_d_and_fit, .options = furrr_options(seed = TRUE))
```

```{r}

#Plot beta posterior for varying priors
ggplot(recovery_df, aes(prior_sd_beta, beta_posterior)) +
  geom_point(alpha = 0.008) +
  geom_hline(yintercept = 0.5, color = "red") +
  geom_smooth(method = lm) +
  facet_wrap(.~prior_sd_bias) +
  theme_classic()

#Plot bias posterior for varying priors
ggplot(recovery_df, aes(prior_sd_bias, bias_posterior)) +
  geom_point(alpha = 0.1) +
  geom_hline(yintercept = 0.5, color = "red") +
  geom_smooth() +
  facet_wrap(.~prior_sd_beta) +
  theme_classic()

```


**Function for simulating with different values for alpha**

```{r}
#Create empty lists and dataframe to append to in the loop
list_of_dfs <- list()
temp <- list()
recovery_df <- data.frame()
#n_trials = 500

#Looping through data simulation, model compilation, and chain checks for varying parameter values of alpha

for(alpha_lvl in seq(0.2, 0.8, 0.2)){ #for alpha values of 0.2, 0.4, 0.6 and 0.8
  set.seed(1982)
  
  alpha = alpha_lvl
  n_trials = 120
  rate <- 0.7 #Probability scale
  beta <- 1 #logodds
  bias <- 0.5 #logodds, small bias towards right 
  
  #Define empty vectors
  Self <- rep(NA, n_trials)
  Other <- rep(NA, n_trials)
  
  #Run simulation
  for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}
  
  #Self - memory agent
  memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)
  
  for (i in 1:n_trials){
    Self[i] <- EMA_Agent_f(beta, bias, memory[i], n_trials)
    memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
  }
  
  
  memory <- memory[1:n_trials]
  
  #Create df
  df <- tibble(Self, Other, memory, trial = seq(n_trials))
  
  
  #Making Self_choice 0 and 1, cause we need that in the bernoulli in the stan model
  df <- df %>% 
    mutate(Self = ifelse(Self == -1, 0, Self))
  
  
  
  #Transform data to lists
  data <- list(
    n_trials = nrow(df),  # n of trials
    choice = df$Self, # sequence of choices
    other = df$Other # sequence of the other agents choices
      )
  
  
  
  file <- file.path("memory_agent.stan")
  mod <- cmdstan_model(file, 
                       # this specifies we can parallelize the gradient estimations on multiple cores
                       cpp_options = list(stan_threads = TRUE), 
                       # this is a trick to make it faster
                       stanc_options = list("O1")) 
  
  #extract samples
  samples <- mod$sample(
    data = data, # the data :-)
    seed = 123,  # a seed, so I always get the same results
    chains = 2,  # how many chains should I fit (to check whether they give the same results)
    parallel_chains = 2, # how many of the chains can be run in parallel?
    threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
    iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
    iter_sampling = 2000, # total number of iterations
    refresh = 0,  # how often to show that iterations have been run
    max_treedepth = 20, # how many steps in the future to check to avoid u-turns
    adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
  )
  
  #extract summary 
  samples$summary()
  
  # Extract posterior samples and include sampling of the prior:
  draws_df <- as_draws_df(samples$draws())

  
  # Checking the model's chains (noise)
  temp = ggplot(draws_df, aes(.iteration, alpha, group = .chain, color = .chain)) +
      geom_line() +
      theme_classic()
  
  # assign function within loop
  assign(paste0("plot_", alpha_lvl), temp)

  assign(paste0("draws_df_", alpha_lvl), draws_df)
  
  recovery_df <- rbind(recovery_df, samples$summary("alpha"), samples$summary("beta"), samples$summary("bias"))
  
}
```

**Looking at the recoveries**
```{r}
recovery_df
```


**Plots for different values of alpha**
```{r}
#install.packages("ggpubr")
library(ggpubr)


#Create plot for alpha 0.2
plot_0.2 <- plot_0.2 + ggtitle("Alpha = 0.2")

#Add a prior for alpha at level 0.2
draws_df_0.2<- draws_df_0.2 %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.2), 1, 1)
)

#Plot the density for alpha (prior and posterior)
p1 <- ggplot(draws_df_0.2) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.2, linetype = "dashed", color = "black", size = 1) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  ylim(0,3)+
  theme_classic()+
  ggtitle("True alpha = 0.2")


#Create plot for alpha 0.4
plot_0.4 <- plot_0.4 + ggtitle("Alpha = 0.4")

#Add a prior for alpha at level 0.4
draws_df_0.4<- draws_df_0.4 %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.4), 1, 1)
)

#Plot the density for alpha (prior and posterior)
p2 <- ggplot(draws_df_0.4) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.4, linetype = "dashed", color = "black", size = 1) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  ylim(0,3)+
  theme_classic()+
  ggtitle("True alpha = 0.4")

#Create plot for alpha 0.6
plot_0.6 <- plot_0.6 + ggtitle("Alpha = 0.6")

#Add a prior for alpha at level 0.6
draws_df_0.6<- draws_df_0.6 %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.6), 1, 1)
)

#Plot the density for alpha (prior and posterior)
p3 <- ggplot(draws_df_0.6) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.6, linetype = "dashed", color = "black", size = 1) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  ylim(0,3)+
  theme_classic()+
  ggtitle("True alpha = 0.6")


#Create plot for alpha 0.8
plot_0.8 <- plot_0.8 + ggtitle("Alpha = 0.8")

#Add a prior for alpha at level 0.8
draws_df_0.8 <- draws_df_0.8 %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.8), 1, 1)
)

#Plot the density for alpha (prior and posterior)
p4 <- ggplot(draws_df_0.8) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.8, linetype = "dashed", color = "black", size = 1) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  ylim(0,3)+
  theme_classic()+
  ggtitle("True alpha = 0.8")
  
#Plot chain convergence for varying alpha levels
ggarrange(plot_0.2 + rremove("x.text")+ rremove("xlab"), plot_0.4 + rremove("x.text")+ rremove("xlab")+ rremove("y.text") + rremove("ylab"), plot_0.6, plot_0.8 + rremove("y.text")+ rremove("ylab"),
          align = "hv",
          padding = 4,
          ncol = 2, nrow = 2)

#Plot densities for prior and posterior for varying alpha levels
ggarrange(p1 + rremove("x.text")+ rremove("xlab"), p2 + rremove("x.text")+ rremove("xlab")+ rremove("y.text") + rremove("ylab"), p3, p4 + rremove("y.text")+ rremove("ylab"),
          align = "hv",
          padding = 4,
          ncol = 2, nrow = 2)

```


**Function for simulating with different values for beta**
```{r}
list_of_dfs <- list()
temp <- list()
recovery_df <- data.frame()
#n_trials = 500

  for(beta_lvl in seq(-1.5, 1.5, 1)){
    set.seed(1982)
    
    beta = beta_lvl
    n_trials = 120
    rate <- 0.7
    alpha <- 0.3 #Cognitively realistic according to the literature 
    bias <- 0.5 #NB this is on a logodds scale, so it has a small bias towards right 
    
    #Define empty vectors
    Self <- rep(NA, n_trials)
    Other <- rep(NA, n_trials)
    
    #Run simulation
    for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}
    
    #Self - memory agent
    memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)
    
    for (i in 1:n_trials){
      Self[i] <- EMA_Agent_f(beta, bias, memory[i], n_trials)
      memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
    }
    
    
    memory <- memory[1:n_trials]
    
    #Create df
    df <- tibble(Self, Other, memory, trial = seq(n_trials))
    
    
    #Making Self_choice 0 and 1, cause we need that in the bernoulli in the stan model
    df <- df %>% 
      mutate(Self = ifelse(Self == -1, 0, Self))
    
    
    
    #Transform data to lists
    data <- list(
      n_trials = nrow(df),  # n of trials
      choice = df$Self, # sequence of choices
      other = df$Other # sequence of the other agents choices
      )
    
    
    
    file <- file.path("memory_agent.stan")
    mod <- cmdstan_model(file, 
                         # this specifies we can parallelize the gradient estimations on multiple cores
                         cpp_options = list(stan_threads = TRUE), 
                         # this is a trick to make it faster
                         stanc_options = list("O1")) 
    
    #extract samples
    samples <- mod$sample(
      data = data, # the data :-)
      seed = 123,  # a seed, so I always get the same results
      chains = 2,  # how many chains should I fit (to check whether they give the same results)
      parallel_chains = 2, # how many of the chains can be run in parallel?
      threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
      iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
      iter_sampling = 2000, # total number of iterations
      refresh = 0,  # how often to show that iterations have been run
      max_treedepth = 20, # how many steps in the future to check to avoid u-turns
      adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
    )
    
    #extract summary 
    samples$summary()
    
    # Extract posterior samples and include sampling of the prior:
    draws_df <- as_draws_df(samples$draws())
    
    temp2 <- tibble(betaEst = (draws_df$beta),
                    betaTrue = beta_lvl,
                    biasTrue = bias_lvl)
    
    if (exists("recovery_df_2")) {recovery_df_2 <- rbind(recovery_df_2, temp2)} else {recovery_df_2 <- temp2}
  
    
    # Checking the model's chains (noise)
    temp = ggplot(draws_df, aes(.iteration, beta, group = .chain, color = .chain)) +
        geom_line() +
        theme_classic()
    
    # assign function within loop
    assign(paste0("plot_", inv_logit_scaled(beta_lvl)), temp)
  
    assign(paste0("draws_df_", inv_logit_scaled(beta_lvl)), draws_df)
    
    
    
    recovery_df <- rbind(recovery_df, samples$summary("alpha"), samples$summary("beta"), samples$summary("bias"))
  }  
```


**Nice plots**
```{r}

plot_0.182425523806356 <- plot_0.182425523806356 + ggtitle("Beta ~ 0.2")
#BETA
# Now let's plot the density for theta (prior and posterior)
p5 <- ggplot(draws_df_0.182425523806356) +
  geom_density(aes(inv_logit_scaled(beta)), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.2, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  theme_classic()+ 
  ylim(0,10)+
  ggtitle("True beta ~ 0.2")

plot_0.377540668798145 <- plot_0.377540668798145 + ggtitle("Beta ~ 0.4")
#BETA
# Now let's plot the density for theta (prior and posterior)
p6<- ggplot(draws_df_0.377540668798145) +
  geom_density(aes(inv_logit_scaled(beta)), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.4, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  ylim(0,10)+
  theme_classic()+ 
  ggtitle("True beta ~ 0.4")


plot_0.622459331201855 <- plot_0.622459331201855 + ggtitle("Beta ~ 0.6")
#BETA
# Now let's plot the density for theta (prior and posterior)
p7 <- ggplot(draws_df_0.622459331201855) +
  geom_density(aes(inv_logit_scaled(beta)), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.6, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  theme_classic()+ 
  ylim(0,10)+
  ggtitle("True beta ~ 0.6")


plot_0.817574476193644 <- plot_0.817574476193644 + ggtitle("Beta ~ 0.8")
#BETA
# Now let's plot the density for theta (prior and posterior)
p8 <- ggplot(draws_df_0.817574476193644) +
  geom_density(aes(inv_logit_scaled(beta)), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.8, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  theme_classic()+ 
  ylim(0,10)+
  ggtitle("True beta ~ 0.8")



library(ggpubr)

ggarrange(plot_0.182425523806356 + rremove("x.text")+ rremove("xlab"), plot_0.377540668798145 + rremove("x.text")+ rremove("xlab")+ rremove("y.text") + rremove("ylab"), plot_0.622459331201855, plot_0.817574476193644 + rremove("y.text")+ rremove("ylab"),
          align = "hv",
          padding = 4,
          ncol = 2, nrow = 2)

ggarrange(p5 + rremove("x.text")+ rremove("xlab"), p6 + rremove("x.text")+ rremove("xlab")+ rremove("y.text") + rremove("ylab"), p7, p8 + rremove("y.text")+ rremove("ylab"),
          align = "hv",
          padding = 4,
          ncol = 2, nrow = 2)



```


**Function for simulating with different values for beta, with an outer loop for bias**
```{r}
list_of_dfs <- list()
temp <- list()
recovery_df <- data.frame()
#n_trials = 500

for (bias_lvl in seq(0, 1.5, 0.5)){
  for(beta_lvl in seq(-1.5, 1.5, 1)){
    set.seed(1982)
    
    beta = beta_lvl
    n_trials = 120
    rate <- 0.7
    alpha <- 0.3 #Cognitively realistic according to the literature 
    bias <- bias_lvl 
    
    #Define empty vectors
    Self <- rep(NA, n_trials)
    Other <- rep(NA, n_trials)
    
    #Run simulation
    for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}
    
    #Self - memory agent
    memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)
    
    for (i in 1:n_trials){
      Self[i] <- EMA_Agent_f(beta, bias, memory[i], n_trials)
      memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
    }
    
    
    memory <- memory[1:n_trials]
    
    #Create df
    df <- tibble(Self, Other, memory, trial = seq(n_trials))
    
    
    #Making Self_choice 0 and 1, cause we need that in the bernoulli in the stan model
    df <- df %>% 
      mutate(Self = ifelse(Self == -1, 0, Self))
    
    
    
    #Transform data to lists
    data <- list(
      n_trials = nrow(df),  # n of trials
      choice = df$Self, # sequence of choices
      other = df$Other # sequence of the other agents choices
      )
    
    
    
    file <- file.path("memory_agent.stan")
    mod <- cmdstan_model(file, 
                         # this specifies we can parallelize the gradient estimations on multiple cores
                         cpp_options = list(stan_threads = TRUE), 
                         # this is a trick to make it faster
                         stanc_options = list("O1")) 
    
    #extract samples
    samples <- mod$sample(
      data = data, # the data :-)
      seed = 123,  # a seed, so I always get the same results
      chains = 2,  # how many chains should I fit (to check whether they give the same results)
      parallel_chains = 2, # how many of the chains can be run in parallel?
      threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
      iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
      iter_sampling = 2000, # total number of iterations
      refresh = 0,  # how often to show that iterations have been run
      max_treedepth = 20, # how many steps in the future to check to avoid u-turns
      adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
    )
    
    #extract summary 
    samples$summary()
    
    # Extract posterior samples and include sampling of the prior:
    draws_df <- as_draws_df(samples$draws())
    
    temp2 <- tibble(betaEst = (draws_df$beta),
                    betaTrue = beta_lvl,
                    biasTrue = bias_lvl)
    
    if (exists("recovery_df_2")) {recovery_df_2 <- rbind(recovery_df_2, temp2)} else {recovery_df_2 <- temp2}
  
    
    # Checking the model's chains (noise)
    temp = ggplot(draws_df, aes(.iteration, beta, group = .chain, color = .chain)) +
        geom_line() +
        theme_classic()
    
    # assign function within loop
    assign(paste0("plot_", inv_logit_scaled(beta_lvl)), temp)
  
    assign(paste0("draws_df_", inv_logit_scaled(beta_lvl)), draws_df)
    
    
    
    recovery_df <- rbind(recovery_df, samples$summary("alpha"), samples$summary("beta"), samples$summary("bias"))
  }  
}
```

