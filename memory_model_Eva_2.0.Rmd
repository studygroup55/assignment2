---
title: "memory_model_Eva.Rmd"
output: html_document
date: "2023-03-23"
---

Prior - posterior update check: generate prior distributions to overlay to the posteriors - what has the model learned from the data?
Prior predictive check: Generate predicted outcomes based on the priors
Posterior predictive check: Generate predicted outcomes based on the posteriors




**Installments** 
```{r}
#Loading packages
pacman::p_load("rstan", "tidyverse", "reshape2", "ggdag", "cmdstanr", "posterior", "brms")
```


**Defining the agents**

```{r}
#Define a random agent
RandomAgent_f <- function(rate){
  choice <- rbinom(1, 1, rate)
  
  if (choice == 0){
    choice = -1
  }
  return(choice)
}

#Memory agent
EMA_Agent_f <- function(beta, bias, memory){
  
  choice <- rbinom(1,1, inv_logit_scaled(bias + beta * memory))
  
  return(choice)
}
```

**Simulation**

```{r}
#Define parameter values
rate <- 0.5
alpha <- 0.2
beta <- 0.5
bias <- 0.5
n_trials <- 120

#Define empty vectors
Self <- rep(NA, n_trials)
Other <- rep(NA, n_trials)

#Other - random agent
for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}

#Self - memory agent
memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)
for (i in 1:n_trials){
  Self[i] <- EMA_Agent_f(beta, bias, memory[i])
  memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
}

```

**Transforming the data to the appropriate form**
```{r}
#Dropping first row in memory vector
memory <- memory[1:120]

#Create df
df <- tibble(Self, Other, memory, trial = seq(n_trials))

#Transform data to lists
data <- list(
  n_trials = nrow(df),  # n of trials
  choice = df$Self, # sequence of choices
  other = df$Other # sequence of the other agents choices
    )
```


**Compile and fit the model**
```{r}
# file <- file.path("memory_agent.stan")
#   mod <- cmdstan_model(file, 
#                        # this specifies we can parallelize the gradient estimations on multiple cores
#                        cpp_options = list(stan_threads = TRUE), 
#                        # this is a trick to make it faster
#                        stanc_options = list("O1")) 
#   
#   #extract samples
#   samples <- mod$sample(
#     data = data, # the data :-)
#     seed = 123,  # a seed, so I always get the same results
#     chains = 2,  # how many chains should I fit (to check whether they give the same results)
#     parallel_chains = 2, # how many of the chains can be run in parallel?
#     threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
#     iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
#     iter_sampling = 2000, # total number of iterations
#     refresh = 0,  # how often to show that iterations have been run
#     max_treedepth = 20, # how many steps in the future to check to avoid u-turns
#     adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
#   )
#   
#   #extract summary 
#   samples$summary()
# 
#   
#   # Extract posterior samples and include sampling of the prior:
#   draws_df <- as_draws_df(samples$draws())
# 
#   
#   # Checking the model's chains (noise)
#   ggplot(draws_df, aes(.iteration, alpha, group = .chain, color = .chain)) +
#       geom_line() +
#       theme_classic()

```


```{r}
pacman::p_load(tidyverse,
        here,
        posterior,
        cmdstanr,
        brms, 
        tidybayes,
        future,
        purrr,
        furrr)
plan(multisession, workers = 4)

#NB: Still not sure I understand how to interpret bias and beta. Ric said that close to 0 is not much, closer to 1 is quite some, over 1 is a lot

#Adding different priors
prior_mean_bias <- 0
prior_sd_bias <- seq(0.2, 0.8, 0.2)
prior_mean_beta <- 0
prior_sd_beta <- seq(0.2, 0.8, 0.2)
#prior_alpha_alpha <- 1
#prior_beta_alpha <- seq(0.2, 0.8, 0.2)

priors <-  tibble(expand.grid(tibble(prior_mean_bias, prior_sd_bias, prior_mean_beta, prior_sd_beta)))


sim_d_and_fit <- function(prior_mean_bias, prior_sd_bias, prior_mean_beta, prior_sd_beta) {
  
  #Transform data to lists
  data <- list(
    n_trials = nrow(df),  # n of trials
    choice = df$Self, # sequence of choices
    other = df$Other, # sequence of the other agents choices
    prior_mean_bias = prior_mean_bias,
    prior_sd_bias = prior_sd_bias,
    prior_mean_beta = prior_mean_beta,
    prior_sd_beta = prior_sd_beta
    )
    
  #Fetch model
  file <- file.path("memory_agent_test.stan")
  mod <- cmdstan_model(file, 
                       # this specifies we can parallelize the gradient estimations on multiple cores
                       cpp_options = list(stan_threads = TRUE), 
                       # this is a trick to make it faster
                       stanc_options = list("O1")) 
  
  samples <- mod$sample(
    data = data,
    seed = 1000,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 1000,
    iter_sampling = 2000,
    refresh = 0,
    max_treedepth = 20,
    adapt_delta = 0.99,
    )
  
  draws_df <- as_draws_df(samples$draws()) 
  
  temp <- tibble(bias_prior = draws_df$bias_prior, 
                 beta_prior = draws_df$beta_prior, 
                 alpha_prior = draws_df$alpha_prior,
                 
                 bias_posterior = draws_df$bias_posterior, 
                 beta_posterior = draws_df$beta_posterior,
                 alpha_posterior = draws_df$alpha_posterior,
                 
                 bias_prior_preds = draws_df$bias_prior_preds,
                 beta_prior_preds = draws_df$beta_prior_preds,
                 alpha_prior_preds = draws_df$alpha_prior_preds,
                 
                 bias_posterior_preds = draws_df$bias_posterior_preds,
                 beta_posterior_preds = draws_df$beta_posterior_preds,
                 alpha_posterior_preds = draws_df$alpha_posterior_preds,
                 
                 prior_mean_bias = prior_mean_bias,
                 prior_sd_bias = prior_sd_bias, 
                 prior_mean_beta = prior_mean_beta,
                 prior_sd_beta = prior_sd_beta)
    
    return(temp)
  
}

```

```{r}

recovery_df <- future_pmap_dfr(priors, sim_d_and_fit, .options = furrr_options(seed = TRUE)) #allows us to map in parallel. Work on multiple inputs and process them in parallel. 



```


```{r}
ggplot(recovery_df, aes(prior_sd_beta, beta_posterior)) +
  geom_point(alpha = 0.008) +
  geom_hline(yintercept = 0.5, color = "red") +
  geom_smooth(method = lm) +
  facet_wrap(.~prior_sd_bias) +
  theme_classic()


# Output shows us that the prior for sd of the bias doesn't matter much for the predictions of beta
# Beta priors for sd approximate the true value the best with a narrow prior
```


```{r}
ggplot(recovery_df, aes(prior_sd_bias, bias_posterior)) +
  geom_point(alpha = 0.1) +
  geom_hline(yintercept = 0.5, color = "red") +
  geom_smooth() +
  facet_wrap(.~prior_sd_beta) +
  theme_classic()
```


















*Function for simulating data with different values for alpha*
Iterate over 6 alpha levels: 0, 0.2, 0.4, 0.6, 0.8, 1.0
Keeping the other parameter values constant






```{r}

temp2 <- list()
recovery_df <- data.frame()
recovery_df_2 <- data.frame()

for (bias_lvl in seq(0, 1.5, 0.5)){
  for(beta_lvl in seq(-1.5, 1.5, 0.5)){
    set.seed(1982)
    
    alpha = 0.5
    n_trials = 120
    rate <- 0.7
    beta <- beta_lvl 
    bias <- bias_lvl #NB this is on a logodds scale, so it has a small bias towards right 
    
    #Define empty vectors
    Self <- rep(NA, n_trials)
    Other <- rep(NA, n_trials)
    
    #Run simulation
    for (t in seq(n_trials)){Other[t] <- RandomAgent_f(rate)}
    
    #Self - memory agent
    memory <- rep(0, n_trials) #create a memory vector with an initial memory value (could be anything)
    
    for (i in 1:n_trials){
      Self[i] <- EMA_Agent_f(beta, bias, memory[i])
      memory[i + 1] <- alpha * memory[i] + (1 - alpha) * Other[i]
    }
    
    
    memory <- memory[1:n_trials]
    
    #Create df
    df <- tibble(Self, Other, memory, trial = seq(n_trials))
    
    
    #Making Self_choice 0 and 1, cause we need that in the bernoulli in the stan model
    df <- df %>% 
      mutate(Self = ifelse(Self == -1, 0, Self))
    
    
    
    #Transform data to lists
    data <- list(
      n_trials = nrow(df),  # n of trials
      choice = df$Self, # sequence of choices
      other = df$Other # sequence of the other agents choices
        )
    
    file <- file.path("memory_agent.stan")
    mod <- cmdstan_model(file, 
                         # this specifies we can parallelize the gradient estimations on multiple cores
                         cpp_options = list(stan_threads = TRUE), 
                         # this is a trick to make it faster
                         stanc_options = list("O1")) 
    
    #extract samples
    samples <- mod$sample(
      data = data, # the data :-)
      seed = 123,  # a seed, so I always get the same results
      chains = 2,  # how many chains should I fit (to check whether they give the same results)
      parallel_chains = 2, # how many of the chains can be run in parallel?
      threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
      iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
      iter_sampling = 2000, # total number of iterations
      refresh = 0,  # how often to show that iterations have been run
      max_treedepth = 20, # how many steps in the future to check to avoid u-turns
      adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
    )
    
    #extract summary 
    samples$summary()
    
    # Extract posterior samples and include sampling of the prior:
    draws_df <- as_draws_df(samples$draws())
    
    temp2 <- tibble(betaEst = (draws_df$beta),
                    betaTrue = beta_lvl,
                    biasTrue = bias_lvl)
    
    if (exists("recovery_df_2")) {recovery_df_2 <- rbind(recovery_df_2, temp2)} else {recovery_df_2 <- temp2}
    
    # Checking the model's chains (noise)
    temp = ggplot(draws_df, aes(.iteration, beta, group = .chain, color = .chain)) +
        geom_line() +
        theme_classic()
    
    # assign function within loop
    assign(paste0("plot_", beta_lvl), temp)
    assign(paste0("draws_df_", beta_lvl), draws_df)
    
    recovery_df <- rbind(recovery_df, samples$summary("alpha"), samples$summary("beta"), samples$summary("bias"))
  }

  
}



unique(temp2$biasTrue)
```




**Plots for different values of beta**
```{r}
plot_0.2

# add a prior for theta (ugly, but we'll do better soon)
draws_df_0.2<- draws_df_0.2 %>% mutate(
  beta_prior = rbeta(nrow(draws_df_0.2), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.2) +
  geom_density(aes(beta), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", beta = 0.3) +
  geom_vline(xintercept = 0.2, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic() + ggtitle("Prior posterior checks, True alpha 0.2")



plot_0.4

# add a prior for theta (ugly, but we'll do better soon)
draws_df_0.4<- draws_df_0.4 %>% mutate(
  beta_prior = rbeta(nrow(draws_df_0.4), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.4) +
  geom_density(aes(beta), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", beta = 0.3) +
  geom_vline(xintercept = 0.4, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic() + ggtitle("Prior posterior checks, True beta 0.4")


plot_0.6

# add a prior for theta (ugly, but we'll do better soon)
draws_df_0.6<- draws_df_0.6 %>% mutate(
  beta_prior = rbeta(nrow(draws_df_0.6), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.6) +
  geom_density(aes(beta), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", beta = 0.3) +
  geom_vline(xintercept = 0.6, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic() + ggtitle("Prior posterior checks, True beta 0.6")


plot_0.8

# add a prior for theta (ugly, but we'll do better soon)
draws_df_0.8 <- draws_df_0.8 %>% mutate(
  beta_prior = rbeta(nrow(draws_df_0.8), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.8) +
  geom_density(aes(beta), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", beta = 0.3) +
  geom_vline(xintercept = 0.8, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()+ ggtitle("Prior posterior checks, True beta 0.8")
```



```{r}

ggplot(recovery_df_2, aes(betaTrue, betaEst)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = 'lm') +
  facet_wrap(.~ biasTrue) +
  theme_classic() + ggtitle("Parameter")
```



```{r}
plot_0.2

#ALPHA
# add a prior for theta (ugly, but we'll do better soon)
draws_df<- draws_df %>% mutate(
  alpha_prior = rbeta(nrow(draws_df_0.2), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.2, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Alpha") +
  ylab("Posterior Density") +
  theme_classic()



#BIAS
# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.2) +
  geom_density(aes(bias), fill = "blue", alpha = 0.3) +
  geom_density(aes(bias_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Bias") +
  ylab("Posterior Density") +
  theme_classic()


#BETA
# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df_0.2) +
  geom_density(aes(beta), fill = "blue", alpha = 0.3) +
  geom_density(aes(beta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.9, linetype = "dashed", color = "black", size = 1.5) +
  xlab("Beta") +
  ylab("Posterior Density") +
  theme_classic()
```



**Looking at the recoveries**
```{r}
recovery_df
```


Assessing prior and posterior predictions
```{r}
#THIS NEEDS A LOT OF LOVE!

ggplot(draws_df_0.2) +
  geom_histogram(aes(alpha_prior_preds), color = "lightblue", fill = "blue", alpha = 0.3, bins = 90) +
  geom_histogram(aes(alpha_posterior_preds), color = "darkblue", fill = "blue", alpha = 0.3, bins = 90) +
  #geom_point(x = sum(data$choice), y = 0, color = "red", shape = 17, size = 5) +
  xlab("Predicted heads out of 1000 trials") +
  ylab("Posterior Density") +
  theme_classic()
```



