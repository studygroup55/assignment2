---
title: "Assignment2_ACM"
author: "Liv Toll√•nes"
date: "2023-02-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#Loading packages
pacman::p_load("reshape2", "tidyverse", "here", "posterior", "cmdstanr", "brms")
```


**1. Setting up a random agent and simulating data**
```{r}
#Defining agent function
RandomAgent_f <- function(rate){
  choice <- rbinom(1, 1, rate)
  return(choice)
}

#Simulating data for radom agent
trials <- 120
rate <- 0.5

#Create empty vector
randomChoice <- rep(NA, trials)
#Run simulation and save output
for (t in seq(trials)) {randomChoice[t] <- RandomAgent_f(rate)}

```


**2. Simulate data for single player - Win-stay-lose-shift agent with noise **

```{r}
#Defining agent function
WSLSAgent_f <- function(prevChoice, Feedback, noise){
  if (Feedback == 1) {      #If feedback = 1 (win), stay
    choice = prevChoice}
  else if (Feedback == 0){ #If feedback = 0 (loss), shift
    choice = 1 - prevChoice}
  if(rbinom(1,1,noise)==1){
    choice <-rbinom(1,1,.5)}
  return(choice)}

WSLSAgent_f(1,0,0.8)


#Define number of trials
trials <- 120

#Create empty vector
WSLSChoice <- rep(NA, trials)

#Simulating data for one player with no opponent - simply generating 
x <- c(0,1)

for (t in seq(trials)){
  WSLSChoice[t] <- WSLSAgent_f(sample(x,1,replace = T), sample(x,1,replace = T),0.7)
}
# 


```


#Random agent agains WSLS agent - one game
- one agent, one opponent, 120 trials
```{r}
ntrials <- 120
rate <- 0.5
noise <- 0.8
self <- rep(NA, ntrials)
other <- rep(NA, ntrials)
feedback_self <- rep(NA,ntrials)

#Randomly setting the first choice on the first trial for the WSLS agent
self[1] <- RandomAgent_f(0.5)

# Simulating data for the random agent
for (t in seq(trials)) {other[t] <- RandomAgent_f(rate)}

for (i in 2:trials){
  if (self[i-1] ==other[i-1]){
    feedback = 1} 
  else {feedback = 0}
  self[i] <-WSLSAgent_f(self[i-1], feedback, noise)}



# #The same simulation-only if we  also want to save the feedback for self
# for (i in 2:trials){
#   if (self[i-1] ==other[i-1]){
#     feedback_self[i] = 1} 
#   else {feedback_self[i] = 0}
#   self[i] <-WSLSAgent_f(self[i-1], feedback_self[i], noise)}


#Transform matrices to long (stacked) format
self_long <- melt(self)

#Adding column names
names(self_long) <- c("self_choices")
#Adding column with trial number
self_long_wtrials <- self_long %>% 
  mutate(Trial_number=row_number())

#Doing the same for other
other_long <- melt(other)
names(other_long) <- c("other_choices")

other_long_wtrials <- other_long %>% 
  mutate(Trial_number=row_number())



#Merge self's andother's choices into one data file 
merged_simple_game <- merge(self_long_wtrials, other_long_wtrials, by = "Trial_number")

```



#Creating a memory agent
```{r}
memory_agent
```




```{r}
#specifying where the model is
file <- file.path("./ass2_test.stan")

mod <- cmdstan_model(stan_file = file, cpp_options = list(stan_threads=TRUE))

data <- write.csv(self_long, "./data.csv") # the input should be a list - not a data frame

data <- as.list(data)

samples <- mod$sample(
  data = data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  refresh = 500,
  max_treedepth = 20,
  adapt_delta = 0.99
)


#when working
# samples$summary()
```


























**Simulate the games for 100 dyads**


1. Random agent vs strategy-shifting agent (33% win-stay-lose-shift, 67% win-shift-lose-stay)
2. WSLS vs strategy-shifting agent (33% win-stay-lose-shift, 66% win-shift-lose-stay)
3. Random agent vs win-shift-shift, lose-stay-stay agent
4. WSLS vs win-shift-shift, lose-stay-stay agent



**1. Random agent vs WSLS-agent, 100 dyads**

```{r}
#Define rate
rate <- 0.5
# Define the number of trials and the number of repetitions
trials <- 120
nsubs <- 100

#Define empty vectors for dealer and player
self <- rep(NA, trials) #player
Other <- rep(NA, trials) #dealer
# Create empty matrices to store the results
self_results <- matrix(0, nrow = trials, ncol = nsubs)
Other_results <- matrix(0, nrow = trials, ncol = nsubs)


#Run simulation
# Outer loop for repetitions
for (j in 1:nsubs) {
  #Define random first choice
  self[1] <- RandomAgent_f(0.5) #player
  for (t in seq(trials)){Other[t] <- RandomAgent_f(rate)}
  
  for (i in 2:trials){
  if (self[i-1] ==other[i-1]){
    feedback = 1} 
  else {feedback = 0}
  self[i] <-WSLSAgent_f(self[i-1], feedback)
  }
  # Store the results in the matrices
  self_results[, j] <- self
other_results[, j] <-other
}


#Transform matrices to long (stacked) format
self_long <- melt(self_results)
Other_long <- melt(Other_results)



#Merge matrices (forother, only column 3, i.e. choices)
merged <- cbind(self_long,other_long[,3])

#Rename dataframe variables
names(merged) <- c("Trial", "Participant", "self_choice", "Other_choice")

#Add feedback
merged$Feedback <- as.numeric(merged$self_choice == merged$Other_choice)

#Add cumulative performance
merged <- merged %>% 
  group_by(Participant) %>% 
  mutate(cumulativerateself = cumsum(Feedback) / seq_along(Feedback),
         cumulativerateOther = cumsum(1-Feedback) / seq_along(Feedback) )

#Plotting with dealer/player colors
ggplot(merged) + theme_classic() + geom_line(color="red", alpha=0.2, aes(x = Trial, y = cumulativerateself, group = Participant)) + geom_line(color="blue", alpha=0.2, aes(x = Trial, y = cumulativerateOther, group = Participant)) + labs(x = "Trial", y = "Cumulative outcome") + ggtitle("Random agent (blue) vs strategy-shifting (red)")

#Plotting according to dyad
#ggplot(merged) + theme_classic() + geom_line(aes(x = Trial, y = cumulativerateself, group = Participant, color = as.factor(Participant))) + geom_line(aes(x = Trial, y = cumulativerateOther, group = Participant, color = as.factor(Participant))) + labs(x = "Trial", y = "Cumulative outcome", color = "Dyad")
#Print mean cumulative rate
#print(mean(merged$cumulativerateself))
#print(mean(merged$cumulativerateOther))
```
