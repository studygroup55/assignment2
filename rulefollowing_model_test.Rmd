

```{r}
#Loading packages
pacman::p_load("rstan", "tidyverse", "reshape2", "ggdag", "cmdstanr", "posterior", "brms")
```

**Defining the agents**
```{r}
#WSLS
WSLSAgent_f <- function(prevChoice, Feedback, ruleFollowing) {
  if (ruleFollowing != 0){
    
    if (prevChoice == 0) {
      prevChoice == -1 }
  
    if (Feedback == 1) { #If feedback = 1 (win), stay
      rate = prevChoice*ruleFollowing
    }
    
    else if (Feedback == 0) { #If feedback = 0 (loss), shift
      rate = (-prevChoice)*ruleFollowing
    }
    
    rate = (1/2)*(rate + 1) #we needed -1 to become 0 and 1 to stay 1, also -0.5 will be 0.25 and 0.5 will be 0.75. 
  }
  
  else { rate = 0.5}
  
  choice <- rbinom(1, 1, rate)

  return(choice)
}


#Defining agent function
RandomAgent_f <- function(rate){
  choice <- rbinom(1, 1, rate)
  return(choice)
}

```


**Simulation**
```{r}
#Define rate
random_rate <- 0.5

#Define ruleFollowing
ruleFollowing <- 0.8

#Define number of trials
trials = 120

#Define empty vectors
Self <- rep(NA, trials)
Other <- rep(NA, trials)

#Define random first choice for player
Self[1] <- RandomAgent_f(0.5) #player

#Run simulation
for (t in seq(trials)){Other[t] <- RandomAgent_f(random_rate)}

for (i in 2:trials){
  if (Self[i-1] == Other[i-1]){
    Feedback = 1
} else {Feedback = 0}
  Self[i] <- WSLSAgent_f(Self[i-1], Feedback, ruleFollowing)
}

#show
df <- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other))
```


**Transforming the data to the appropriate form**
```{r}
#empty column
df$strategy_choice <- NA

#Transform
df$strategy_choice[df$Other == 1 & df$Self == 1] <- 1 #self wins on right, goes right
df$strategy_choice[df$Other == 1 & df$Self == 0] <- 1 #self loses on left, goes right
df$strategy_choice[df$Other == 0 & df$Self == 1] <- -1 #self loses on right, goes left
df$strategy_choice[df$Other == 0 & df$Self == 0] <- -1 #self wins on left, goes left

#df$Other[df$Other == 0 ] <- -1 #self wins on right, goes right
#df$Self[df$Self == 0] <- -1 

insert_value = NA

#shift strategy choice one down
df = df %>%
   mutate(strategy_choice = lag(strategy_choice, n = 1, default = insert_value))

#exclude first trial
df = df[-1,]

df$Feedback <- NA


#Transform to list
data <- list(
  n_trials = nrow(df),  # n of trials
  choice = df$Self, # sequence of choices
  strategy_choice = df$strategy_choice # whether or not agent is choosing right 
)

data
```


**Compile and fit the model**
```{r}
#collect stan model
file <- file.path("rulefollowing_agent.stan")
mod <- cmdstan_model(file, 
                     # this specifies we can parallelize the gradient estimations on multiple cores
                     cpp_options = list(stan_threads = TRUE), 
                     # this is a trick to make it faster
                     stanc_options = list("O1")) 

#extract samples
samples <- mod$sample(
  data = data, # the data :-)
  seed = 123,  # a seed, so I always get the same results
  chains = 2,  # how many chains should I fit (to check whether they give the same results)
  parallel_chains = 2, # how many of the chains can be run in parallel?
  threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
  iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
  iter_sampling = 2000, # total number of iterations
  refresh = 0,  # how often to show that iterations have been run
  max_treedepth = 20, # how many steps in the future to check to avoid u-turns
  adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
)

#extract summary 
samples$summary()

samples$summary("ruleFollowing")

# Extract posterior samples and include sampling of the prior:
draws_df <- as_draws_df(samples$draws())

# Checking the model's chains (noise)
ggplot(draws_df, aes(.iteration,  ruleFollowing, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()

# Checking the model's chains (rate)
ggplot(draws_df, aes(.iteration, rate, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()

```


**Running model with prior sensitivity analysis**
```{r}
pacman::p_load(tidyverse,
        here,
        posterior,
        cmdstanr,
        brms, 
        tidybayes,
        future,
        purrr,
        furrr)
plan(multisession, workers = 4)

#Adding different priors
prior_mean_rulefollowing <- 0 #do we want different means?
prior_sd_rulefollowing <- seq(0.2, 0.8, 0.2)

priors <-  tibble(expand.grid(tibble(prior_mean_rulefollowing, prior_sd_rulefollowing)))

sim_d_and_fit <- function(prior_mean_rulefollowing, prior_sd_rulefollowing) {
  
  #Transform data to lists
  data <- list(
  n_trials = nrow(df),  # n of trials
  choice = df$Self, # sequence of choices
  strategy_choice = df$strategy_choice, # whether or not agent is choosing right
  prior_mean_rulefollowing = prior_mean_rulefollowing,
  prior_sd_rulefollowing = prior_sd_rulefollowing
  )
    
  #Fetch model
  file <- file.path("rulefollowing_agent_test.stan")
  mod <- cmdstan_model(file, 
                       # this specifies we can parallelize the gradient estimations on multiple cores
                       cpp_options = list(stan_threads = TRUE), 
                       # this is a trick to make it faster
                       stanc_options = list("O1")) 
  
  samples <- mod$sample(
    data = data,
    seed = 1000,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 1000,
    iter_sampling = 2000,
    refresh = 0,
    max_treedepth = 20,
    adapt_delta = 0.99,
    )
  
  draws_df <- as_draws_df(samples$draws()) 
  
  temp <- tibble(rulefollowing_prior = draws_df$rulefollowing_prior, 
                 
                 rulefollowing_posterior = draws_df$rulefollowing_posterior, 
                 
                 rulefollowing_prior_preds = draws_df$rulefollowing_prior_preds,
                 
                 rulefollowing_posterior_preds = draws_df$bias_posterior_preds,
 
                 prior_mean_rulefollowing = prior_mean_rulefollowing,
                 prior_sd_rulefollowing = prior_sd_rulefollowing)
    
    return(temp)
  
}
```



**Apply future map**
```{r}

recovery_df <- future_pmap_dfr(priors, sim_d_and_fit, .options = furrr_options(seed = TRUE))

```






